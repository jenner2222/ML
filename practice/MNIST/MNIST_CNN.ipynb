{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ConvNet for MNIST Classification\n",
    "\n",
    "This example creates a Convolutional Neural Network to classify handwritten digits in the MNIST dataset. ConvNets are a powerful network architecture to create features from images. Below, we will define several network architectures and examine their performance on this well-known dataset. The deep ConvNet produced an error below 0.50% .\n",
    "\n",
    "This exercise is based on a [Machine Learning Mastery tutorial](http://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras). A good resource for ConvNets is the [CS231n pages on GitHub](http://cs231n.github.io/convolutional-networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/16\n",
      "30s - loss: 0.4409 - acc: 0.8561 - val_loss: 0.0730 - val_acc: 0.9773\n",
      "Epoch 2/16\n",
      "28s - loss: 0.1253 - acc: 0.9624 - val_loss: 0.0489 - val_acc: 0.9852\n",
      "Epoch 3/16\n",
      "28s - loss: 0.0905 - acc: 0.9729 - val_loss: 0.0347 - val_acc: 0.9886\n",
      "Epoch 4/16\n",
      "28s - loss: 0.0715 - acc: 0.9785 - val_loss: 0.0278 - val_acc: 0.9909\n",
      "Epoch 5/16\n",
      "28s - loss: 0.0608 - acc: 0.9824 - val_loss: 0.0248 - val_acc: 0.9924\n",
      "Epoch 6/16\n",
      "28s - loss: 0.0559 - acc: 0.9833 - val_loss: 0.0218 - val_acc: 0.9932\n",
      "Epoch 7/16\n",
      "28s - loss: 0.0510 - acc: 0.9847 - val_loss: 0.0227 - val_acc: 0.9922\n",
      "Epoch 8/16\n",
      "28s - loss: 0.0460 - acc: 0.9865 - val_loss: 0.0198 - val_acc: 0.9942\n",
      "Epoch 9/16\n",
      "28s - loss: 0.0443 - acc: 0.9867 - val_loss: 0.0213 - val_acc: 0.9939\n",
      "Epoch 10/16\n",
      "28s - loss: 0.0393 - acc: 0.9883 - val_loss: 0.0211 - val_acc: 0.9938\n",
      "Epoch 11/16\n",
      "28s - loss: 0.0393 - acc: 0.9882 - val_loss: 0.0177 - val_acc: 0.9951\n",
      "Epoch 12/16\n",
      "28s - loss: 0.0376 - acc: 0.9887 - val_loss: 0.0171 - val_acc: 0.9946\n",
      "Epoch 13/16\n",
      "28s - loss: 0.0368 - acc: 0.9891 - val_loss: 0.0207 - val_acc: 0.9942\n",
      "Epoch 14/16\n",
      "28s - loss: 0.0331 - acc: 0.9897 - val_loss: 0.0195 - val_acc: 0.9948\n",
      "Epoch 15/16\n",
      "28s - loss: 0.0320 - acc: 0.9905 - val_loss: 0.0199 - val_acc: 0.9941\n",
      "Epoch 16/16\n",
      "28s - loss: 0.0292 - acc: 0.9911 - val_loss: 0.0168 - val_acc: 0.9955\n",
      "Deep Error: 0.45%\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 43\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "def baseline_model():\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Convolution2D(32, 5, 5, border_mode='valid', input_shape=(1, 28, 28), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "def larger_model():\n",
    "\tmodel = Sequential()\n",
    "    # create 30 features with a 5x5 kernel\n",
    "\tmodel.add(Convolution2D(50, 5, 5, border_mode='valid', input_shape=(1, 28, 28), activation='relu'))\n",
    "    # reduce the spatial size of our first conv layer, reducing the number of weights and thereby overfitting\n",
    "    # generally, strides of > 2 are too destructive\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2))) # takes a default stride size of 2, reducing input to 1x14x14\n",
    "\tmodel.add(Convolution2D(15, 3, 3, activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Dense(50, activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "def deep_model():\n",
    "    # based on the pattern: INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL]*3 -> [FC -> RELU]*2 -> FC\n",
    "    # from the CS231n treatment of ConvNet architectures.\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Convolution2D(40, 3, 3, border_mode='valid', input_shape=(1, 28, 28), activation='relu'))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Convolution2D(40, 3, 3, activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Convolution2D(20, 3, 3, activation='relu'))\n",
    "\tmodel.add(Convolution2D(20, 3, 3, activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(256, activation='relu'))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Dense(50, activation='relu'))\n",
    "\tmodel.add(Dropout(0.1))\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# build the model\n",
    "model = deep_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=16, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Deep Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
